# End-to-End Credit Scoring System: A Production-Ready MVP

This project is a Minimum Viable Product (MVP) for an end-to-end credit scoring system. While the current implementation uses lightweight, local-first tools for rapid development, it is built on a modular, scalable architecture designed for a seamless transition to a production-grade environment.

## API Documentation

The system includes a mock API that simulates the [Flinks API](https://docs.flinks.com/docs/welcome) for development and testing purposes.

## High-Level Architecture

The system is composed of four main components, each designed to be modular and independently scalable. This separation of concerns is key to the system's flexibility, allowing individual components to be upgraded or replaced without impacting the rest of the system.

### 1. Mock API (`api_mock`)

- **Purpose**: Mock implementation of a subset of the [Flinks API](https://docs.flinks.com/docs/welcome?_gl=1*je0k2v*_gcl_au*NDY3Mzk0Mzc5LjE3NTEwNDk5NzU.). The user can either access the frontend and download the CSV files for a given customer, or request the data via the API. For comprehensive API documentation with detailed endpoints and curl examples, see [docs/api_mock.md](docs/api_mock.md).
    <img width="1728" alt="image" src="https://github.com/user-attachments/assets/0d284991-03a5-4b71-ad39-e4a69a8a5069" />
- **Technology**: A Flask-based API that serves mock bank statement data from CSV files.
- **Production Strategy**: These pipelines can be easily migrated to a production environment and deployed on a cloud-based workflow orchestrator like Apache Airflow or Prefect. The data lake itself would be moved to a cloud storage solution like Amazon S3 or Google Cloud Storage.

### 2. Data Pipelines (`pipelines`)

- **Purpose**: A set of robust data pipelines responsible for ingesting, cleaning, and transforming the raw data into a usable format.
- **Technology**: The pipelines are built using Python with `deltalake` and `pandas`, processing data in stages and storing the output in a local, transactional data lake.
- **Architecture**: The data lake is located at the root directory  at the location `data_lake/` and follows a standard multi-hop architecture:
    - **Bronze Layer**: Raw, unprocessed data is landed here from the source API.
    - **Silver Layer**: Data is cleaned, standardized, and enriched.
    - **Application Status Ledger**: A dedicated table (`application_status_ledger`) tracks the state of each application through the workflow.
- **Key Feature: Idempotency & ACID Transactions**: The entire data lake is built on **Delta Lake**. Instead of manual state management, the pipelines leverage atomic `MERGE` operations. This provides ACID guarantees and ensures that every pipeline run is idempotent, preventing data duplication and corruption. This is a modern, industry-standard approach to building reliable data platforms.
- **Production Strategy**: These pipelines can be easily migrated to a production environment and deployed on a cloud-based workflow orchestrator like Apache Airflow or Prefect. The data lake itself would be moved to a cloud storage solution like Amazon S3 or Google Cloud Storage.

### 3. Analytics (`analytics`)

- **Purpose**: This component is responsible for running analytics on the cleaned data to generate insights and features for the credit scoring model.
- **Technology**: A dbt (Data Build Tool) project that uses DuckDB as its data warehouse. This allows for rapid, SQL-based development of data models and transformations on the local Parquet files.
  <img width="1679" alt="image" src="https://github.com/user-attachments/assets/d577adea-af2b-4820-9f3f-e5eae0a596c1" />
- **Production Strategy**: In a production setting, this dbt project would be reconfigured to connect to a cloud data warehouse like Snowflake, BigQuery, or Redshift, enabling it to handle much larger datasets and more complex analytical workloads.

### 4. Taktile (Not Implemented)

- **Purpose**: This future component will be responsible for the final credit decisioning.
- **Implementation Plan**: It will take the features generated by the analytics layer and send them to a decisioning engine (like Taktile) to get a credit score and recommendation. The results will then be stored back in our data warehouse for analysis and monitoring.

## Path to Production

The current MVP is fully functional for local development and testing. The following steps outline the path to a full production deployment:

1.  **Swap the Mock API**: Reconfigure the `api_mock` client to connect to the live Flinks API.
2.  **Deploy Pipelines**: Deploy the data pipelines to a workflow orchestrator like **Apache Airflow** for scheduled, reliable execution.
3.  **Scale the Data Lake**: Migrate the data lake from local storage to a cloud object store like **Amazon S3** or **Google Cloud Storage**.
4.  **Upgrade the Data Warehouse**: Transition the analytics from DuckDB to a production data warehouse like **Snowflake**, **BigQuery**, or **Redshift** to handle large-scale data.
5.  **Implement the Decisioning Logic**: Build out the Taktile integration to connect the feature data to the live decisioning engine.

## Getting Started

### Prerequisites
- Docker & Docker Compose
- Python 3.10+ (for local script execution)

### Running the End-to-End Demo

This workflow demonstrates the entire system, from starting the API to running the data pipelines and the analytics models.

**Step 1: Start the Services**

First, start the mock API service using Docker Compose. This will also build the necessary container image.

```bash
docker-compose up --build
```
Leave this running in a terminal. The API will now be available at `http://127.0.0.1:5000`.

**Step 2: Run the Data Pipelines**

In a **new terminal**, you can run the data pipelines. If you haven't already, install the Python dependencies:
```bash
pip install -r requirements.txt
```

Now, run the pipelines sequentially.

1.  **Run the ingestion pipeline** to fetch data from the API and populate the bronze layer:
    ```bash
    python -m pipelines.ingest_statements
    ```
2.  **Run the transformation pipeline** to clean the data and update the silver and ledger tables:
    ```bash
    python -m pipelines.transform_statements
    ```

**Step 3: Run the Analytics Models**

With the data processed, you can now run the dbt models to generate the final analytics tables.

1.  Navigate to the analytics directory:
    ```bash
    cd analytics
    ```
2.  Run the dbt models:
    ```bash
    dbt run
    ```

After completing these steps, you will have successfully executed the entire data flow from end to end. You can explore the final tables in the `pipelines/data_lake/analytics` directory.
