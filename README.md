# End-to-End Credit Scoring System

This project implements a comprehensive, end-to-end credit scoring system, designed with production-readiness and scalability in mind. It ingests raw financial data, processes it through a series of data pipelines, and prepares it for credit decisioning.

## High-Level Architecture

The system is composed of four main components, each designed to be modular and independently scalable.

### 1. Mock API (`api_mock`)

- **Purpose**: Mimics the behavior of a third-party financial data aggregator (like Flinks), providing realistic statement data for development and testing.
- **Technology**: A Flask-based API that serves mock bank statement data from CSV files.
- **Production Strategy**: This component is designed to be easily swappable. In a production environment, the API client would be reconfigured to point to the real Flinks API endpoint with minimal changes to the downstream data pipelines.

### 2. Data Pipelines (`pipelines`)

- **Purpose**: A set of robust data pipelines responsible for ingesting, cleaning, and transforming the raw data into a usable format.
- **Technology**: The pipelines are built using Python and Pandas, processing the data in stages and storing the output as Parquet files in a local data lake.
- **Architecture**:
    - **Bronze Layer**: Raw, unprocessed data is ingested from the API and landed in the `data_lake/bronze` directory.
    - **Silver Layer**: The raw data is then cleaned, validated, and transformed into a standardized schema, with the results stored in the `data_lake/silver` directory.
- **Key Feature: Idempotency**: A critical feature of these pipelines is their idempotency, which is achieved through a `scoring_status.py` ledger. This stateful component tracks the processing status (`PENDING`, `SENT`, `SCORED`, `ERROR`) of each transaction. By checking the ledger before processing, the system guarantees that no transaction is ever processed more than once, even if the pipeline is run multiple times. This prevents data duplication and ensures the integrity of the analytics downstreamâ€”a crucial requirement for any financial system.
- **Production Strategy**: These pipelines can be easily migrated to a production environment and deployed on a cloud-based workflow orchestrator like Apache Airflow or Prefect. The data lake itself would be moved to a cloud storage solution like Amazon S3 or Google Cloud Storage.

### 3. Analytics (`analytics`)

- **Purpose**: This component is responsible for running analytics on the cleaned data to generate insights and features for the credit scoring model.
- **Technology**: A dbt (Data Build Tool) project that uses DuckDB as its data warehouse. This allows for rapid, SQL-based development of data models and transformations on the local Parquet files.
- **Production Strategy**: In a production setting, this dbt project would be reconfigured to connect to a cloud data warehouse like Snowflake, BigQuery, or Redshift, enabling it to handle much larger datasets and more complex analytical workloads.

### 4. Taktile (Not Implemented)

- **Purpose**: This future component will be responsible for the final credit decisioning.
- **Implementation Plan**: It will take the features generated by the analytics layer and send them to a decisioning engine (like Taktile) to get a credit score and recommendation. The results will then be stored back in our data warehouse for analysis and monitoring.

## How to Run the Project

The entire project is containerized using Docker and can be easily run with a single command:

```bash
docker-compose up --build
``` 
