services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
      target: api
    ports:
      - "5000:5000"
    volumes:
      # Mount the local 'app' directory into the container's 'app' directory
      - ./api_mock:/app/api_mock
      - ./config.json:/app/config.json
      - ./run.py:/app/run.py
    environment:
      - FLASK_APP=run.py
      # This tells Flask to run in development mode
      - FLASK_ENV=development

  underwriter_app:
    build:
      context: .
      dockerfile: Dockerfile
      target: underwriter_app
    ports:
      - "8501:8501"
    volumes:
      # Mount the code and config so changes are reflected without rebuilding
      - ./app:/app
      - ./config.json:/app/config.json
      - ./pipelines:/app/pipelines
      - ./analytics:/app/analytics
      # Mount the data lake directory to persist data artifacts
      - ./data_lake:/app/data_lake
    depends_on:
      - api
    env_file:
      - .env
    environment:
      # This ensures dbt in the container writes to the host filesystem via the volume mount
      - PWD=/app
      # This variable tells the application where the root directory is inside the container.
      - DATA_LAKE_ROOT=/app

  dbt_docs:
    build:
      context: .
      dockerfile: Dockerfile
      target: dbt_docs
    ports:
      - "8081:8081"
    volumes:
      # Mount the analytics project and data lake so docs can be generated
      # against the current state of the data.
      - ./analytics:/app/analytics
      - ./data_lake:/app/data_lake
    depends_on:
      - api
    environment:
      - DATA_LAKE_ROOT=/app

  jupyter:
    build:
      context: .
      dockerfile: Dockerfile
      target: jupyter
    ports:
      - "8888:8888"
    volumes:
      # Mount all relevant project directories for full access
      - ./analytics:/app/analytics
      - ./data_lake:/app/data_lake
      - ./pipelines:/app/pipelines
      - ./api_mock:/app/api_mock
      - ./docs:/app/docs
    depends_on:
      - api
